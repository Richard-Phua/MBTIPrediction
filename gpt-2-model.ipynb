{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":8702450,"sourceType":"datasetVersion","datasetId":5219490},{"sourceId":8713905,"sourceType":"datasetVersion","datasetId":5227812}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Define New Dataset\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(4)\n\ndf = pd.read_csv('/kaggle/input/undersampling-mbti-df/mbti_TF_undersampling.csv')\ndf_train, df_test = np.split(df.sample(frac=1, random_state=35),\n                                     [int(0.9*len(df))])\nprint(len(df_train), len(df_test))\ndf_train_1 = df_train.filter(['TF','post'], axis=1)\ndf_test_1 = df_test.filter(['TF','post'], axis=1)\n\n\ndf_train_1.to_csv(\"/kaggle/working/train_kaggle_DF.csv\")\ndf_test_1.to_csv(\"/kaggle/working/test_kaggle_DF.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import set_seed, GPT2Config, GPT2Tokenizer, GPT2ForSequenceClassification\n\nset_seed(731) \n\nmodel_config = GPT2Config.from_pretrained('gpt2', num_labels=2) # Binary Classification\nmodel = GPT2ForSequenceClassification.from_pretrained('gpt2', config=model_config)\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.padding_side = \"left\" \ntokenizer.pad_token = tokenizer.eos_token\n\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.config.pad_token_id = model.config.eos_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Build Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom torch.utils.data import Dataset\n\nclass MBTIDataset(Dataset):\n    def __init__(self, train=True):\n        super().__init__()\n        self.train = train\n        self.data = pd.read_csv(os.path.join('/kaggle/working/', 'train_kaggle_DF.csv' if train else 'test_kaggle_DF.csv'))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        record = self.data.iloc[index]\n        text = record['post']\n        if self.train:\n            return {'text': text, 'label': record['JP']}\n        else:\n            return {'text': text, 'label': '0'}\n\ntrain_dataset = MBTIDataset(train=True)\ntest_dataset = MBTIDataset(train=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    print(train_dataset.__getitem__(i)['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Data Collator","metadata":{}},{"cell_type":"code","source":"class Gpt2ClassificationCollator(object):\n    def __init__(self, tokenizer, max_seq_len=None):\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n        \n        return\n    \n    def __call__(self, sequences):\n        texts = [sequence['text'] for sequence in sequences]\n        labels = [int(sequence['label']) for sequence in sequences]\n        inputs = self.tokenizer(text=texts,\n                                return_tensors='pt',\n                                padding=True,\n                                truncation=True,\n                                max_length=self.max_seq_len)\n        inputs.update({'labels': torch.tensor(labels)})\n        \n        return inputs\n\ngpt2classificationcollator = Gpt2ClassificationCollator(tokenizer=tokenizer,\n                                                        max_seq_len=60)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. DataLoader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\n\ntrain_size = int(len(train_dataset) * 0.9)\nval_size = len(train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\ntrain_dataloader = DataLoader(dataset=train_dataset,\n                              batch_size=16,\n                              shuffle=True,\n                              collate_fn=gpt2classificationcollator)\nval_dataloader = DataLoader(dataset=val_dataset,\n                            batch_size=16,\n                            shuffle=False,\n                            collate_fn=gpt2classificationcollator)\ntest_dataloader = DataLoader(dataset=test_dataset,\n                             batch_size=16,\n                             shuffle=False,\n                             collate_fn=gpt2classificationcollator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Optimizer & Lr Scheduler","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW, get_cosine_schedule_with_warmup\n\ntotal_epochs = 5\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\noptimizer = AdamW(optimizer_grouped_parameters,\n                  lr=1e-5,\n                  eps=1e-8)\n\nnum_train_steps = len(train_dataloader) * total_epochs\nnum_warmup_steps = int(num_train_steps * 0.1) \n\nlr_scheduler = get_cosine_schedule_with_warmup(optimizer,\n                                               num_warmup_steps=num_warmup_steps,\n                                               num_training_steps = num_train_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Train & Validation","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef train(dataloader, optimizer, scheduler, device_):\n    global model\n    model.train()\n    \n    prediction_labels = []\n    true_labels = []\n    \n    total_loss = []\n    \n    for batch in dataloader:\n        true_labels += batch['labels'].numpy().flatten().tolist()\n        batch = {k:v.type(torch.long).to(device_) for k, v in batch.items()}\n        \n        \n        outputs = model(**batch)\n        loss, logits = outputs[:2]\n        logits = logits.detach().cpu().numpy()\n        total_loss.append(loss.item())\n        \n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # prevent exploding gradient\n\n        optimizer.step()\n        scheduler.step()\n        \n        prediction_labels += logits.argmax(axis=-1).flatten().tolist()\n    \n    return true_labels, prediction_labels, total_loss\n\ndef validation(dataloader, device_):\n    global model\n    model.eval()\n    \n    prediction_labels = []\n    true_labels = []\n    \n    total_loss = []\n    \n    for batch in dataloader:\n        true_labels += batch['labels'].numpy().flatten().tolist()\n        batch = {k:v.type(torch.long).to(device_) for k, v in batch.items()}\n        \n        with torch.no_grad():\n            outputs = model(**batch)\n            loss, logits = outputs[:2]\n            logits = logits.detach().cpu().numpy()\n            total_loss.append(loss.item())\n\n            prediction_labels += logits.argmax(axis=-1).flatten().tolist()\n        \n    return true_labels, prediction_labels, total_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. Run!","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, f1_score\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\n\nall_loss = {'train_loss': [], 'val_loss': []}\nall_acc = {'train_acc': [], 'val_acc': []}\n\nfor epoch in range(total_epochs):\n    y, y_pred, train_loss = train(train_dataloader, optimizer, lr_scheduler, device)\n    train_acc = accuracy_score(y, y_pred)\n    \n    y, y_pred, val_loss = validation(val_dataloader, device)\n    val_acc = accuracy_score(y, y_pred)\n    recall = recall_score(y, y_pred)\n    precision = precision_score(y, y_pred)\n    f1 = f1_score(y, y_pred)\n    \n    all_loss['train_loss'] += train_loss\n    all_loss['val_loss'] += val_loss\n    \n    all_acc['train_acc'].append(train_acc)\n    all_acc['val_acc'].append(val_acc)\n    \n    print(f'Epoch: {epoch}, train_loss: {torch.tensor(train_loss).mean():.3f}, train_acc: {train_acc:.3f}, val_loss: {torch.tensor(val_loss).mean():.3f}, val_acc: {val_acc:.3f}')\n    print(f'Epoch: {epoch}, recall: {recall:.3f}, precision: {precision:.3f},f1: {f1:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfig = plt.figure(figsize=(20,20))\na = fig.add_subplot(4, 1, 1)\nb = fig.add_subplot(4, 1, 2)\nc = fig.add_subplot(2, 1, 2)\na.plot(all_loss['train_loss'])\nb.plot(all_loss['val_loss'])\nc.plot(all_acc['train_acc'])\nc.plot(all_acc['val_acc'])\nc.set(xlabel='epoch', ylabel='accuracy')\nc.legend(['train', 'val'])\n\npass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8. Do for others Dichotomies","metadata":{}},{"cell_type":"code","source":"class MBTIDataset(Dataset):\n    def __init__(self, train=True):\n        super().__init__()\n        self.train = train\n        self.data = pd.read_csv(os.path.join('/kaggle/working/', 'train_kaggle_DF.csv' if train else 'test_kaggle_DF.csv'))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        record = self.data.iloc[index]\n        text = record['post']\n        if self.train:\n            return {'text': text, 'label': record['EI']}\n        else:\n            return {'text': text, 'label': '0'}\n\ntrain_dataset = MBTIDataset(train=True)\ntest_dataset = MBTIDataset(train=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EI","metadata":{}},{"cell_type":"code","source":"all_loss = {'train_loss': [], 'val_loss': []}\nall_acc = {'train_acc': [], 'val_acc': []}\n\nfor epoch in range(total_epochs):\n    y, y_pred, train_loss = train(train_dataloader, optimizer, lr_scheduler, device)\n    train_acc = accuracy_score(y, y_pred)\n    \n    y, y_pred, val_loss = validation(val_dataloader, device)\n    val_acc = accuracy_score(y, y_pred)\n    recall = recall_score(y, y_pred)\n    precision = precision_score(y, y_pred)\n    f1 = f1_score(y, y_pred)\n     \n    \n    all_loss['train_loss'] += train_loss\n    all_loss['val_loss'] += val_loss\n    \n    all_acc['train_acc'].append(train_acc)\n    all_acc['val_acc'].append(val_acc)\n    \n    print(f'Epoch: {epoch}, train_loss: {torch.tensor(train_loss).mean():.3f}, train_acc: {train_acc:.3f}, val_loss: {torch.tensor(val_loss).mean():.3f}, val_acc: {val_acc:.3f}') \n    print(f'Epoch: {epoch}, recall: {recall:.3f}, precision: {precision:.3f},f1: {f1:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SN","metadata":{}},{"cell_type":"code","source":"all_loss = {'train_loss': [], 'val_loss': []}\nall_acc = {'train_acc': [], 'val_acc': []}\n\nfor epoch in range(total_epochs):\n    y, y_pred, train_loss = train(train_dataloader, optimizer, lr_scheduler, device)\n    train_acc = accuracy_score(y, y_pred)\n    \n    y, y_pred, val_loss = validation(val_dataloader, device)\n    val_acc = accuracy_score(y, y_pred)\n    recall = recall_score(y, y_pred)\n    precision = precision_score(y, y_pred)\n    f1 = f1_score(y, y_pred)\n     \n    \n    all_loss['train_loss'] += train_loss\n    all_loss['val_loss'] += val_loss\n    \n    all_acc['train_acc'].append(train_acc)\n    all_acc['val_acc'].append(val_acc)\n    \n    print(f'Epoch: {epoch}, train_loss: {torch.tensor(train_loss).mean():.3f}, train_acc: {train_acc:.3f}, val_loss: {torch.tensor(val_loss).mean():.3f}, val_acc: {val_acc:.3f}') \n    print(f'Epoch: {epoch}, recall: {recall:.3f}, precision: {precision:.3f},f1: {f1:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF","metadata":{}},{"cell_type":"code","source":"all_loss = {'train_loss': [], 'val_loss': []}\nall_acc = {'train_acc': [], 'val_acc': []}\n\nfor epoch in range(total_epochs):\n    y, y_pred, train_loss = train(train_dataloader, optimizer, lr_scheduler, device)\n    train_acc = accuracy_score(y, y_pred)\n    \n    y, y_pred, val_loss = validation(val_dataloader, device)\n    val_acc = accuracy_score(y, y_pred)\n    recall = recall_score(y, y_pred)\n    precision = precision_score(y, y_pred)\n    f1 = f1_score(y, y_pred)\n     \n    \n    all_loss['train_loss'] += train_loss\n    all_loss['val_loss'] += val_loss\n    \n    all_acc['train_acc'].append(train_acc)\n    all_acc['val_acc'].append(val_acc)\n    \n    print(f'Epoch: {epoch}, train_loss: {torch.tensor(train_loss).mean():.3f}, train_acc: {train_acc:.3f}, val_loss: {torch.tensor(val_loss).mean():.3f}, val_acc: {val_acc:.3f}') \n    print(f'Epoch: {epoch}, recall: {recall:.3f}, precision: {precision:.3f},f1: {f1:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}